---
layout: post
title: "ElasticSearch ELK日志分析"
date: 2017-12-14
author: silly
categories: ["工具"]
desc: "ELK日志分析!"
tags: ["ElasticSearch"]
permalink: "/tool/elk.html"
---

ELK由Elasticsearch、Logstash和Kibana三部分组件组成；

Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。

Logstash是一个完全开源的工具，它可以对你的日志进行收集、分析，并将其存储供以后使用

kibana 是一个开源和免费的工具，它可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。

本文将用本地安装模式安装集群, 如果有条件, 请使用云平台如AWS上的服务.

我的环境是Ubuntu16.04, 并且安装了Java环境, 安装Java参见[This](/tool/java-install.html). ElasticSearch是由Java开发的哦.

# 一.安装ElasticSearch

## 1.1 单机运行

从[官网](https://www.elastic.co/cn/downloads/elasticsearch)下载`tar`包:

```
# 解压
tar -zxvf elasticsearch-6.1.0.tar.gz

# 新建文件夹
mkdir -p $HOME/escluster

# 移动到我们的新文件夹中
mv elasticsearch-6.1.0 $HOME/escluster/es1

# 尝试单机运行
cd $HOME/escluster/es1
./bin/elasticsearch -V

./bin/elasticsearch
```

打开`http://127.0.0.1:9200` 可以观察是否成功.

测试:

```sh
curl -XPUT 'http://localhost:9200/twitter/doc/1?pretty' -H 'Content-Type: application/json' -d '
{
    "user": "kimchy",
    "post_date": "2009-11-15T13:12:00",
    "message": "Trying out Elasticsearch, so far so good?"
}'

curl -XGET 'http://localhost:9200/twitter/doc/1?pretty=true'
```

## 1.2 集群安装
ElasticSearch ELK日志分析
以上只是测试是否初步安装成功, 我们要搭建集群, 因为`ElasticSearch`是分布式全文搜索引擎.

安装包下有很多目录:

|目录|作用|
|----|----|
|bin |主要是各种启动命令|
|config|主要存放配置文件|
|data|该节点的索引存放目录, 可改|
|lib|es依赖的一些依赖包|
|logs|日志存放目录, 可改|
|plugins|es强大的插件系统|

我们在本机装个两个节点的集群.

我们修改`elasticsearch.yml`来做集群, 相关解释已经在注释中, 部分参考[ElasticSearch单机双实例的配置方法](http://blog.csdn.net/lw305080/article/details/52528731).

```sh
cd $HOME/escluster/es1
vim config/elasticsearch.yml 

# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster: 标明了整个集群的名字，只有节点在相同的集群在能互相发现。
#
cluster.name: my-application
#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node: 当前节点名称的标识，各个节点的名称不能重复, 切记!
#
node.name: node-1
#
# Add custom attributes to the node:
#
#node.attr.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
# 新建文件夹, 将数据和日志定位到$HOME/testes中
# sudo mkdir /app/testes
# 将该文件夹赋予执行ES的用户 
# sudo chown jinhan testes
# sudo chgrp jinhan testes
# 不同节点node的保存路径应该不一样
# mkdir /app/testes/node1
# mkdir /app/testes/node2
# 下面必须写全路径
path.data: /app/testes/node1/data
#
# Path to log files:
#
path.logs: /app/testes/node1/logs
#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
#bootstrap.memory_lock: true
#
# Make sure that the heap size is set to about half the memory available
# on the system and that the owner of the process is allowed to use this
# limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# Set the bind address to a specific IP (IPv4 or IPv6): 指定为本机IP, 否则可能导致外部无法访问
#
network.host: 127.0.0.1
#
# Set a custom port for HTTP: 节点间通信端口, 节点在不同机器可以一样, 但单机模拟集群, 不能一样
#
http.port: 9200
transport.tcp.port: 9300

# 本地集群必须设置, 切记!!!
# 这个配置限制了单节点上可以开启的ES存储实例的个数，我们需要开多个实例，因此需要把这个配置写到配置文件中，并为这个配置赋值为2或者更高。
node.max_local_storage_nodes: 2
#
# For more information, consult the network module documentation.
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when new node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
#
# 由于到了2.x版本之后，ES取消了默认的广播模式来发现master节点，需要使用该配置来指定发现master节点。这个配置在单机双实例的配置中需要特别注意下，因为习惯上我们配置时并未指定master节点的tcp端口，如果实例的transport.tcp.port配置为9301，那么实例启动后会认为discovery.zen.ping.unicast.hosts中指定的主机tcp端口也是9301，可能导致这些节点无法找到master节点。因此在该配置中需要指定master节点提供服务的tcp端口。
#discovery.zen.ping.unicast.hosts: ["host1", "host2"]
discovery.zen.ping.unicast.hosts: ["127.0.0.1:9300","127.0.0.1:9301"]

# es配置当前集群中最少的主节点数，对于多于两个节点的集群环境，建议配置大于1。我们的节点目前没有多于两个, 所以不设置
# discovery.zen.minimum_master_nodes: 2
#
# Prevent the "split brain" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):
#
#discovery.zen.minimum_master_nodes: 
#
# For more information, consult the zen discovery module documentation.
#
# ---------------------------------- Gateway -----------------------------------
#
# Block initial recovery after a full cluster restart until N nodes are started:
#
#gateway.recover_after_nodes: 3
#
# For more information, consult the gateway module documentation.
#
# ---------------------------------- Various -----------------------------------
#
# Require explicit names when deleting indices:
#
#action.destructive_requires_name: true
```

 我们再拷贝一份:

```
cp -r $HOME/escluster/es1 $HOME/escluster/es2
```

将另一份的配置改为:
 
 `path.data: /app/testes/node1/data`改为`path.data: /app/testes/node2/data`,
 `path.logs: /app/testes/node1/logs`改为`path.logs: /app/testes/node2/logs`,
 `node.name: node-1`改为`node.name: node-2`,
 `http.port: 9200`改为`http.port: 9201`, 
 `transport.tcp.port: 9300`改为`transport.tcp.port: 9301`

跑起来(跑起来后关掉一个再连, 集群还会保持):

```
$HOME/escluster/es1/bin/elasticsearch -d
$HOME/escluster/es2/bin/elasticsearch -d
```

访问: `http://127.0.0.1:9200`和`http://127.0.0.1:9201`

```
GET 127.0.0.1:9200/_cluster/health
```

PS: 也可以不复制整个安装包成两份, 只需复制两份配置, 然后更改配置, 运行:

```
$HOME/escluster/es1/bin/elasticsearch -Des.path.conf=config/instance1 -d -p /tmp/elasticsearch_1.pid
$HOME/escluster/es1/bin/elasticsearch -Des.path.conf=config/instance2 -d -p /tmp/elasticsearch_2.pid
```

更多参考: [ElasticSearch源码地址](https://github.com/elastic/elasticsearch)

## 1.3 脑裂问题

参考: http://blog.csdn.net/cnweike/article/details/39083089

正常情况下，集群中的所有的节点，应该对集群中master的选择是一致的，这样获得的状态信息也应该是一致的，不一致的状态信息，说明不同的节点对master节点的选择出现了异常——也就是所谓的脑裂问题。这样的脑裂状态直接让节点失去了集群的正确状态，导致集群不能正常工作。

可能导致的原因：

1. 网络：由于是内网通信，网络通信问题造成某些节点认为master死掉，而另选master的可能性较小；进而检查Ganglia集群监控，也没有发现异常的内网流量，故此原因可以排除。
2. 节点负载：由于master节点与data节点都是混合在一起的，所以当工作节点的负载较大（确实也较大）时，导致对应的ES实例停止响应，而这台服务器如果正充当着master节点的身份，那么一部分节点就会认为这个master节点失效了，故重新选举新的节点，这时就出现了脑裂；同时由于data节点上ES进程占用的内存较大，较大规模的内存回收操作也能造成ES进程失去响应。所以，这个原因的可能性应该是最大的。

应对问题的办法：

1.对应于上面的分析，推测出原因应该是由于节点负载导致了master进程停止响应，继而导致了部分节点对于master的选择出现了分歧。为此，一个直观的解决方案便是将master节点与data节点分离。为此，我们添加了三台服务器进入ES集群，不过它们的角色只是master节点，不担任存储和搜索的角色，故它们是相对轻量级的进程。可以通过以下配置来限制其角色：

```
node.master: true  
node.data: false  
```

当然，其它的节点就不能再担任master了，把上面的配置反过来即可。这样就做到了将master节点与data节点分离。当然，为了使新加入的节点快速确定master位置，可以将data节点的默认的master发现方式由multicast修改为unicast, 这两种是ES的默认自动发现（Disovery）：

```
discovery.zen.ping.multicast.enabled: false  
discovery.zen.ping.unicast.hosts: ["master1", "master2", "master3"]
```

2.还有两个直观的参数可以减缓脑裂问题的出现：

discovery.zen.ping_timeout（默认值是3秒）：默认情况下，一个节点会认为，如果master节点在3秒之内没有应答，那么这个节点就是死掉了，而增加这个值，会增加节点等待响应的时间，从一定程度上会减少误判。

discovery.zen.minimum_master_nodes（默认是1）：这个参数控制的是，一个节点需要看到的具有master节点资格的最小数量，然后才能在集群中做操作。官方的推荐值是(N/2)+1，其中N是具有master资格的节点的数量（我们的情况是3，因此这个参数设置为2，但对于只有2个节点的情况，设置为2就有些问题了，一个节点DOWN掉后，你肯定连不上2台服务器了，这点需要注意）

# 二.安装Kibana

从[官网](https://www.elastic.co/cn/downloads/kibana)下载`tar`包.

解压并修改配置

```
tar -zxvf kibana-6.1.0-linux-x86_64.tar.gz 
cd kibana-6.1.0-linux-x86_64
vim config/kibana.yml

elasticsearch.url: "http://127.0.0.1:9200"
```

运行:

```
bin/kibana

# 后台运行

nohup bin/kibana &
```

打开: `http://127.0.0.1:5601` 即可.

# 三.使用Logstash

Logstash是一个数据分析软件，主要目的是分析log日志。整一套软件可以当作一个MVC模型，logstash是controller层，Elasticsearch是一个model层，kibana是view层。

首先将数据传给logstash，它将数据进行过滤和格式化（转成JSON格式），然后传给Elasticsearch进行存储、建搜索的索引，kibana提供前端的页面再进行搜索和图表可视化，它是调用Elasticsearch的接口返回的数据进行可视化。logstash和Elasticsearch是用Java写的，kibana使用node.js框架。

这里是全文重点.

## 3.1 安装

从[官网](https://www.elastic.co/downloads/logstash)下载`tar`包.

```
tar -zxvf logstash-6.1.0.tar.gz 
cd logstash-6.1.0
```

## 3.2 运行

定义一个简单的示例日志收集处理配置`logstash.conf`:

```
input { stdin { } }
output {
  elasticsearch { hosts => ["127.0.0.1:9200"] }
  stdout { codec => rubydebug }
}
```

这里`input`从标准输入`stdin`接收日志, `output`将日志输出到`elasticsearch`, stdout.codecs是基于数据流的过滤器，它可以作为input，output的一部分配置。Codecs可以帮助你轻松的分割发送过来已经被序列化的数据。流行的codecs包括 json,msgpack,plain(text)。

开始跑:

```
bin/logstash -f logstash.conf

> 随便打
```

打开`kibana`来分析: http://127.0.0.1:5601/app/kibana#/management/kibana/index?_g=()

![](/picture/public/elk1.png)

![](/picture/public/elk2.png)


接着可以打开`Discover`进行日志查看:

![](/picture/public/elk.png)

也可以进`Visualize`进行`Create a visualization`操作:

![](/picture/public/elk3.png)


# 四. 开始日志分析实战

参见: https://www.cnblogs.com/yincheng/p/logstash.html

Go语言日志库使用: http://www.jianshu.com/p/5fac8bed4505

Go语言支持: https://github.com/bshuster-repo/logrus-logstash-hook

待写, 期待中....