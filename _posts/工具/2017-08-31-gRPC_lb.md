---
layout: post
title: "gRPC的使用/protobuffer/负载均衡"
date: 2017-08-31
author: silly
categories: ["工具"]
desc: "这是对gRPC的一些理解, 包括gRPC的使用, 负载均衡灰度发布等"
tags: ["gRPC"]
permalink: "/tool/gRPC.html"
---

# 0.胡邹

以前开发网站都是用http协议,学过TCP/IP协议的人都知道,在传输层TCP的基础上,应用层HTTP就是填充了一定规则的文本.

# 1.gRPC使用和介绍

工作中使用到gRPC,其实http请求也是一种rpc变种,远程进程调用.gRPC底层是HTTP2协议

gRPC一开始由google开发，是一款语言中立、平台中立、开源的远程过程调用(RPC)系统，面向移动和HTTP/2设计。目前提供C、Java和Go语言版本，分别是：grpc,grpc-java,grpc-go.其中C版本支持C,C++,Node.js,Python,Ruby,Objective-C,PHP和C#支持.

gRPC基于HTTP/2标准设计，带来诸如双向流、流控、头部压缩、单TCP连接上的多复用请求等特。这些特性使得其在移动设备上表现更好，更省电和节省空间占用。

gRPC基于以下理念：定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个gRPC服务器来处理客户端调用。在客户端拥有一个存根能够像服务端一样的方法,即调用仿佛就在同一台机器。

可以使用不同语言平台进行开发

![](/picture/grpc/rpc.png)


gRPC 默认使用protocol buffers来进行消息通讯，这是Google开源的一套成熟的结构数据序列化机制

参考： [gRPC 官方文档中文版](http://doc.oschina.net/grpc) | [概念](http://doc.oschina.net/grpc?t=58009)

## Protocol

Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.

项目地址：[https://github.com/google/protobuf](https://github.com/google/protobuf)

安装参考： [https://github.com/google/protobuf/blob/master/src/README.md](https://github.com/google/protobuf/blob/master/src/README.md)

安装`The protocol compiler`

```
sudo apt-get install autoconf automake libtool curl make g++ unzip
./autogen.sh
####
#执行./autogen.sh时，报error: configure.ac:1: file'gtest/m4/acx_pthread.m4' does not exist的错误。
#在gmock目录新建gtest文件夹，拷贝项目根目录下m4文件夹至gtest文件夹
####
./configure
make
make check
sudo make install
sudo ldconfig # refresh shared library cache.
```

使用(下面go语言还需安装插件)

```
go get -u github.com/golang/protobuf/proto
go get -u github.com/golang/protobuf/protoc-gen-go
```

文档参考： [protocol-buffers文档](https://developers.google.com/protocol-buffers/)

## Go gRPC

Go版库：[https://github.com/grpc/grpc-go](https://github.com/grpc/grpc-go)

获取：

```
go get google.golang.org/grpc
```

使用

```
protoc --go_out=plugins=grpc:. *.proto
```

如果要网关转http,请

```
go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway
go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-swagger
```

然后

```
#!/usr/bin/env bash
protoc -I/usr/local/include -I. \
  -I$GOPATH/src \
  -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \
  --go_out=plugins=grpc:. \
  msg_newest.proto

protoc -I/usr/local/include -I. \
  -I$GOPATH/src \
  -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \
  --grpc-gateway_out=logtostderr=true:. \
  msg_newest.proto
```

## Python gRPC

```
pip install grpcio==1.4.0
pip install grpcio-tools==1.4.0
pip install protobuf

python -m grpc.tools.protoc --python_out=pbdata --grpc_python_out=pbdata -I . msg.proto
```

# 2.灰度发布和负载均衡

因为gRPC有服务器端和客户端,服务器可以用Go写,客户端啥语言都可以写.然后,我的服务器端要重启,影响到了客户端的使用!...

我如何保存链接,进行灰度发布.

玛格但,查了好久,查不到,发现其实灰度发布其实可以用负载均衡来实现.

淡淡: 见了:https://segmentfault.com/a/1190000008672912 感觉思路很好,想摘一摘,自己写个LB.

> 构建高可用、高性能的通信服务，通常采用服务注册与发现、负载均衡和容错处理等机制实现。根据负载均衡实现所在的位置不同，通常可分为以下三种解决方案：

## 集中式LB（Proxy Model）

![](/picture/grpc/lb1.png)

在服务消费者和服务提供者之间有一个独立的LB，通常是专门的硬件设备如 F5，或者基于软件如 LVS，HAproxy等实现。LB上有所有服务的地址映射表，通常由运维配置注册，当服务消费方调用某个目标服务时，它向LB发起请求，由LB以某种策略，比如轮询（Round-Robin）做负载均衡后将请求转发到目标服务。LB一般具备健康检查能力，能自动摘除不健康的服务实例。 该方案主要问题：

1. 单点问题，所有服务调用流量都经过LB，当服务数量和调用量大的时候，LB容易成为瓶颈，且一旦LB发生故障影响整个系统；
2. 服务消费方、提供方之间增加了一级，有一定性能开销。

自己的理解:

类似于Nginx负载均衡,类似于JAVA AOP,一个过滤器,本来客户端直接和服务器通信,现在客户端要先通过中介,而且,这个中介只有一个,中介如何告知服务器端的位置,全靠脑洞(此集中式没说如何注册服务),所以中介压力好大,中介会被逼死,所以出现以下两种!

## 进程内LB（Balancing-aware Client）

![](/picture/grpc/lb2.png)

针对第一个方案的不足，此方案将LB的功能集成到服务消费方进程里，也被称为软负载或者客户端负载方案。服务提供方启动时，首先将服务地址注册到服务注册表，同时定期报心跳到服务注册表以表明服务的存活状态，相当于健康检查，服务消费方要访问某个服务时，它通过内置的LB组件向服务注册表查询，同时缓存并定期刷新目标服务地址列表，然后以某种负载均衡策略选择一个目标服务地址，最后向目标服务发起请求。LB和服务发现能力被分散到每一个服务消费者的进程内部，同时服务消费方和服务提供方之间是直接调用，没有额外开销，性能比较好。该方案主要问题：

1. 开发成本，该方案将服务调用方集成到客户端的进程里头，如果有多种不同的语言栈，就要配合开发多种不同的客户端，有一定的研发和维护成本；
2. 另外生产环境中，后续如果要对客户库进行升级，势必要求服务调用方修改代码并重新发布，升级较复杂。

自己的理解:

中介现在分销了,往每个客户端分配一个中介,全程负责.但是中介大BOSS其实还是一个,就是注册局,只不过这个注册局现在不是一个人了,而是一个类似于数据库如etcd.每个服务器端启动时会心跳的和注册局沟通,注册局知道你down了没有,每个客户端要请求连接时,其身边的中介会向注册局请示.然而,这个是有问题的,客户可能是英国人,可能是印度人,我擦,中介还要培训再派过去.好,现在出现第三种,我拆出来:

## 独立 LB 进程（External Load Balancing Service）

![](/picture/grpc/lb3.png)

该方案是针对第二种方案的不足而提出的一种折中方案，原理和第二种方案基本类似。

不同之处是将LB和服务发现功能从进程内移出来，变成主机上的一个独立进程。主机上的一个或者多个服务要访问目标服务时，他们都通过同一主机上的独立LB进程做服务发现和负载均衡。该方案也是一种分布式方案没有单点问题，一个LB进程挂了只影响该主机上的服务调用方，服务调用方和LB之间是进程内调用性能好，同时该方案还简化了服务调用方，不需要为不同语言开发客户库，LB的升级不需要服务调用方改代码。

该方案主要问题：部署较复杂，环节多，出错调试排查问题不方便。

自己的理解:

方法是人想的,说来说去都差不多,囧,不要说差远了....